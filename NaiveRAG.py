import streamlit as st
import spacy
import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Load spaCy model
nlp = spacy.load("en_core_web_sm")
class NaiveRAG:
    def __init__(self):
        print('Hello in Naive RAG')
    def get_synonyms(self,word):
        synonyms = set()
        for syn in wordnet.synsets(word):
            for lemma in syn.lemmas():
                synonyms.add(lemma.name())
        return synonyms

    def preprocess_text(self,text):
        doc = nlp(text.lower())
        lemmatized_words = []
        for token in doc:
            if token.is_stop or token.is_punct:
                continue
            lemmatized_words.append(token.lemma_)
        return lemmatized_words

    def expand_with_synonyms(self,words):
        expanded_words = words.copy()
        for word in words:
            expanded_words.extend(self.get_synonyms(word))
        return expanded_words

    def calculate_enhanced_similarity(self,text1, text2):
        # Preprocess and tokenize texts
        words1 = self.preprocess_text(text1)
        words2 = self.preprocess_text(text2)

        # Expand with synonyms
        words1_expanded = self.expand_with_synonyms(words1)
        words2_expanded = self.expand_with_synonyms(words2)

        # Count word frequencies
        freq1 = Counter(words1_expanded)
        freq2 = Counter(words2_expanded)

        # Create a set of all unique words
        unique_words = set(freq1.keys()).union(set(freq2.keys()))

        # Create frequency vectors
        vector1 = [freq1[word] for word in unique_words]
        vector2 = [freq2[word] for word in unique_words]

        # Convert lists to numpy arrays
        vector1 = np.array(vector1)
        vector2 = np.array(vector2)

        # Calculate cosine similarity
        cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))

        return cosine_similarity

    def find_best_match_keyword_search(self,query, db_records):
        best_score = 0
        best_record = None

        # Split the query into individual keywords
        query_keywords = set(query.lower().split())

        # Iterate through each record in db_records
        for record in db_records:
            # Split the record into keywords
            record_keywords = set(record.lower().split())

            # Calculate the number of common keywords
            common_keywords = query_keywords.intersection(record_keywords)
            current_score = len(common_keywords)

            # Update the best score and record if the current score is higher
            if current_score > best_score:
                best_score = current_score
                best_record = record

        return best_score, best_record

    def calculate_cosine_similarity(self,text1, text2):
        vectorizer = TfidfVectorizer(
            stop_words='english',
            use_idf=True,
            norm='l2',
            ngram_range=(1, 2),  # Use unigrams and bigrams
            sublinear_tf=True,  # Apply sublinear TF scaling
            analyzer='word'  # You could also experiment with 'char' or 'char_wb' for character-level features
        )
        tfidf = vectorizer.fit_transform([text1, text2])
        similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])
        return similarity[0][0]